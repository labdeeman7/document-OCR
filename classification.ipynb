{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep learning classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh9ZWF0qxMQ5",
        "colab_type": "text"
      },
      "source": [
        "#Solution for classification of documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajOjHZ-FxXS9",
        "colab_type": "text"
      },
      "source": [
        "##1.1 Explanation\n",
        "We would be using various algorithms to try and classify our classes between letters, memos and forms\n",
        "\n",
        "As usual, we have to get our extracted data fro google drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3AMCfC0x4Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"title contains '.zip'\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEPylqfPyGdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1lJYTmX_PUyUVDSWvAiFkmoEJWZNA3msH'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('extractedData.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPdsHUCbyd9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip extractedData.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjI4HFkCyl8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "path = os.getcwd()  \n",
        "print (\"The current working directory is {}\" .format(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PslwjnLDyyGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = [\"Email\",\"Letter\",\"Memo\"]; \n",
        "dataFolders = [\"train\",\"test\",\"val\"];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re0QpwBC0-Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-UgOrL2bUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEznmBSk6g11",
        "colab_type": "text"
      },
      "source": [
        "##1.2 Machine learning algorithms\n",
        "We would be using the naive bayes algorithm and the SGD classifier  to classify the documents\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMLx5eaj2bbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For reproducibility\n",
        "np.random.seed(1237)\n",
        " \n",
        "# train file directory\n",
        "path_train = os.path.join(path, \"extractedData\", \"train\")\n",
        " \n",
        "files_train = skds.load_files(path_train,load_content=False)\n",
        " \n",
        "label_index = files_train.target\n",
        "label_names = files_train.target_names\n",
        "labelled_files = files_train.filenames\n",
        "\n",
        " \n",
        "data_tags = [\"filename\",\"category\",\"text\"]\n",
        "data_list = []\n",
        " \n",
        "# Read and add data from file to a list\n",
        "i=0\n",
        "for f in labelled_files:\n",
        "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
        "    i += 1\n",
        " \n",
        "# # We have training data available as dictionary filename, category, data\n",
        "extractedtrainData = pd.DataFrame.from_records(data_list, columns=data_tags)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy9rpTbD2be6",
        "colab_type": "code",
        "outputId": "8a125cd7-01b7-426a-df3f-9a44b48762e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "# test file directory\n",
        "path_test = os.path.join(path, \"extractedData\", \"test\")\n",
        " \n",
        "files_test = skds.load_files(path_test,load_content=False)\n",
        " \n",
        "test_label_index = files_test.target\n",
        "test_label_names = files_test.target_names\n",
        "test_labelled_files = files_test.filenames\n",
        "\n",
        "\n",
        "test_data_text = []\n",
        " \n",
        "# Read and add data from file to a list\n",
        "\n",
        "for f in test_labelled_files:\n",
        "    test_data_text.append(Path(f).read_text())\n",
        "    i += 1\n",
        "    \n",
        "print(test_labelled_files[0])\n",
        "print(test_label_names)\n",
        "print(len(test_data_text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/extractedData/test/Letter/10230391.txt\n",
            "['Email', 'Letter', 'Memo']\n",
            "268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dxqikRu2bi3",
        "colab_type": "code",
        "outputId": "a75e927b-e2fb-43e6-fbb7-f1f9b6c25dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "extractedtrainData.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/extractedData/train/Letter/517783754+...</td>\n",
              "      <td>Letter</td>\n",
              "      <td>@\\n\\n \\n\\n».DEC-04-97 THU 11:17 AM NB BULK PAC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/extractedData/train/Memo/0000936338.txt</td>\n",
              "      <td>Memo</td>\n",
              "      <td>Ts oe were eo rrr * Po Mi \"\\n/\\n™ x a ‘ a\\nTE ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/extractedData/train/Email/2083182848d...</td>\n",
              "      <td>Email</td>\n",
              "      <td>— Original Message-----\\n\\nFrom: Dawson, Rober...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/extractedData/train/Memo/04107922_041...</td>\n",
              "      <td>Memo</td>\n",
              "      <td>: Lorillard\\n\\nMEMORANDUM\\nJuly 27, 1982\\nTO: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/extractedData/train/Email/2070106870a...</td>\n",
              "      <td>Email</td>\n",
              "      <td>oo Original Message-----\\n\\nFrom: Arce, Miguel...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            filename  ...                                               text\n",
              "0  /content/extractedData/train/Letter/517783754+...  ...  @\\n\\n \\n\\n».DEC-04-97 THU 11:17 AM NB BULK PAC...\n",
              "1   /content/extractedData/train/Memo/0000936338.txt  ...  Ts oe were eo rrr * Po Mi \"\\n/\\n™ x a ‘ a\\nTE ...\n",
              "2  /content/extractedData/train/Email/2083182848d...  ...  — Original Message-----\\n\\nFrom: Dawson, Rober...\n",
              "3  /content/extractedData/train/Memo/04107922_041...  ...  : Lorillard\\n\\nMEMORANDUM\\nJuly 27, 1982\\nTO: ...\n",
              "4  /content/extractedData/train/Email/2070106870a...  ...  oo Original Message-----\\n\\nFrom: Arce, Miguel...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiSz9NYQ2bn7",
        "colab_type": "code",
        "outputId": "d7e4ecf4-0e0c-471c-f777-fdd62a3a0eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_text = extractedtrainData['text']\n",
        "train_tags = extractedtrainData['category']\n",
        "train_files_names = extractedtrainData['filename']\n",
        "\n",
        "train_text.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1249,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtcpMUTk2bqz",
        "colab_type": "code",
        "outputId": "67cb1c9b-330a-48fc-f1c0-cf46a91bea98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform(train_text)\n",
        "vectors.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1249, 25752)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSInwr7P2btW",
        "colab_type": "code",
        "outputId": "33daacbd-9918-4b7b-9abd-89e29d4e28fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# using the example from sklearn\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "clf = MultinomialNB(alpha=.01)\n",
        "clf.fit(vectors, label_index)\n",
        "\n",
        "# newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "vectors_test = vectorizer.transform(test_data_text)\n",
        "\n",
        "pred = clf.predict(vectors_test)\n",
        "# np.mean(pred == newsgroups_test.target)\n",
        "metrics.f1_score(test_label_index, pred, average='macro')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8841200077471059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVJzKNm22bv0",
        "colab_type": "code",
        "outputId": "0eec56dc-4c80-4208-d7aa-381fe47071af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf_svm = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, random_state=42))])\n",
        "\n",
        "text_clf_svm = text_clf_svm.fit(train_text, label_index)\n",
        "predicted_svm = text_clf_svm.predict(test_data_text)\n",
        "\n",
        "np.mean(predicted_svm == test_label_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9029850746268657"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gsNm1KxTDn5",
        "colab_type": "text"
      },
      "source": [
        "Save the sgd classifier model and test saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m2pWhfPTDD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "892151e2-c432-4542-e958-37a11e6af3e9"
      },
      "source": [
        "pickle.dump(text_clf_svm, open('model.pkl','wb'))\n",
        "\n",
        "model = pickle.load(open('model.pkl','rb'))\n",
        "print(model.predict([validation_sentence]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJ8RyS9Eb_P",
        "colab_type": "text"
      },
      "source": [
        "##1.3 Prediction using the validation test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLI1eJoaEl9w",
        "colab_type": "code",
        "outputId": "a699f162-47d7-49a6-df7e-4ca702dd0e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "path_val = os.path.join(path, \"extractedData\", \"val\")\n",
        "path_val\n",
        "\n",
        "files_val = skds.load_files(path_val,load_content=False)\n",
        " \n",
        "val_label_index = files_val.target\n",
        "val_label_names = files_val.target_names\n",
        "val_labelled_files = files_val.filenames\n",
        "\n",
        "\n",
        "print(val_labelled_files[0])\n",
        "print(val_label_names)\n",
        "\n",
        "validation_sentence = Path(val_labelled_files[0]).read_text()\n",
        "validation_sentence\n",
        "\n",
        "vectors_val = vectorizer.transform([validation_sentence])\n",
        "\n",
        "# Prediction for the naive bayes classifier\n",
        "pred = clf.predict(vectors_val)\n",
        "print(pred)\n",
        "\n",
        "# Prediction for the SGD classifier\n",
        "predicted_svm = text_clf_svm.predict([validation_sentence])\n",
        "print(predicted_svm)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/extractedData/val/Memo/85418261.txt\n",
            "['Email', 'Letter', 'Memo']\n",
            "[2]\n",
            "[2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5mF_dGqSd7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx2d8NRmDALz",
        "colab_type": "text"
      },
      "source": [
        "##1.4 Deep learning methodologies for classifciation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xR8WOyfz9zR",
        "colab_type": "code",
        "outputId": "687021ca-6cd8-4d6a-c965-7e8e34008379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import sklearn.datasets as skds\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9ZoTHh8zGQV",
        "colab_type": "code",
        "outputId": "a4214614-c5ef-450b-84bc-c9fd6e39fc49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# For reproducibility\n",
        "np.random.seed(1237)\n",
        " \n",
        "# train file directory\n",
        "path_train = os.path.join(path, \"extractedData\", \"train\")\n",
        " \n",
        "files_train = skds.load_files(path_train,load_content=False)\n",
        " \n",
        "train_label_index = files_train.target\n",
        "train_label_names = files_train.target_names\n",
        "train_labelled_files = files_train.filenames\n",
        "\n",
        " \n",
        "train_labelled_text = []\n",
        "\n",
        "# Read and add data from file to a list\n",
        "i=0\n",
        "\n",
        "for file in train_labelled_files:\n",
        "  with open(file, 'r') as f:\n",
        "    text = f.read() \n",
        "    train_labelled_text.append(text)\n",
        "  i += 1\n",
        "    \n",
        "print(len(train_label_index))\n",
        "print(train_label_index[:10])\n",
        "\n",
        "print(train_label_names)\n",
        "print(train_labelled_text[:10])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1249\n",
            "[1 2 0 2 0 2 0 0 2 2]\n",
            "['Email', 'Letter', 'Memo']\n",
            "[\"@\\n\\n \\n\\n\\xc2\\xbb.DEC-04-97 THU 11:17 AM NB BULK PACKAGING FAX NO. 7704293533 P, 05/08\\n\\nUnited States Food Safety Washington, 0.C.\\nDepartment of and Inspection 20250\\nAgriculture Service\\n\\nEgg Products Inspection Division\\nJune 14, 1996\\n\\nMr. Gary Brown\\nMacMillan Bloedel Bulk Packaging\\nDivision of MacMillan Bloedel of America, Inc.\\n\\n1811 West Oak Parkway\\nMarietta, Georgia 30062\\n\\nDear Mr, Brown:\\n\\nThis is in response to your June 4 letter requesting Department of Agriculmre (USDA)\\napproval of the polypropylene Tre Blue Ball Valve, manufactured by BLDL, Inc, Totowa,\\nNew Jersey, for use in plants operating under the federal egg products inspection program.\\nYou propose to use the apparatus as an outlet valve for attachment to your firm's single-use\\ncorrugated fiber IBC portable container approved for use in official egg products plants,\\nEnclosed with your letter were specifications and drawings describing the construction of the\\nBLDL ball valve,\\n\\nBased on the information and drawing submitted and your written certification that the ball\\nvalve is fabricated of materials that comply with Federal Food and Drug Administration\\nregulations, Title 21 CFR 177.1520, we are approving the polypropylene BLDL True Blue\\nValve as a component outlet valve on your firm's IBC corrugated fiber container. This\\napproval for use of the polypropylene valve in official egg products plants is subject to the\\nfollowing provisions:,\\n\\n1. The ball valve and stainless steel bag-puncture apparatus are completely\\ndisassembled after each use for cleaning and inspection.\\n\\n2. Evidence of scoring or damage to the product contact surfaces of the ball valve\\naffecting cleanabillty will result in termination of approval for use of an individual valve\\nassembly, ,\\n\\n3. As stated in previous discussions, corrugated fiber totes are not approved for\\nstorage of liquid egg products in official egg products. Therefore, upon installation of the\\nvalve on the container, the contents must be removed on a continuous basis until the container\\n\\nis empty.\\n\\nFailure to comply with the above provisions will result in termination of this approval.\\nIf you have any additional questions, please contact this office.\\nSincerely,\\n\\n\\xe2\\x80\\x9cEge A tnaakgy\\n\\nRoger L. Glasshoff\\nNational Supervisor, Plant Operations\\n\\nPME FOR 2OI0- 8 0 EN? BQUAL CPPCATUNITY IN EMPLOYMENT AND SESVICES\\n\\nPSLE SLLTS\", 'Ts oe were eo rrr * Po Mi \"\\n/\\n\\xe2\\x84\\xa2 x a \\xe2\\x80\\x98 a\\nTE ST a aE cer NERO eer gene nelnmenmUERUsE ernie Lees\\n-\\xe2\\x84\\xa2 o~\\n. _\\n1\\nMEMORANDUM\\nTO: MR. E. PEPPLES\\n- FROM: L, A, PIZER\\nDATE: October 2, 1974\\nSUBJECT: VICEROY\\nCy Attached for your review are the following advertising materials;\\nyet\\xe2\\x80\\x9d v-NW-74-1 \"Driver - 2 Pack\" Jewish & Motor Trade _ Stat\\n\\xe2\\x80\\x9d Proof\\nae > Proef\\nMay we have your comments concerning the above?\\n. L. A. P.\\nye ~~\\n; biped\\n\\xe2\\x80\\x98+\\njo\\no A \\xe2\\x80\\x98\\nt\\n\\na\\n\\nerm se UN eee\\ns@3MAM-THIS NOTICE. IT 1$:DUE.TO THE Vn nore) 7\\nQUALITY OF THE DOCUMENT BEING FILMED.', '\\xe2\\x80\\x94 Original Message-----\\n\\nFrom: Dawson, Robert G. [mailto:robert.g.dawson@wawa.com]\\nSent: Monday, June 19, 2000 5:22 PM\\n\\nTo: \\'Nellenbach, Jack\\xe2\\x80\\x99\\n\\nSubject: FW: Pledge 2000 \"We Card\" Meetings in Delaware\\n\\nJack, FYI.\\n\\nBob\\n\\nC8P878T E807', ': Lorillard\\n\\nMEMORANDUM\\nJuly 27, 1982\\nTO: Messrs. C. H. Judge\\nJ. R. Ave\\nFROM: T. H. Mau\\n\\nSUBJECT: Kent Family Advertising Recommendation\\n\\nThe attached memoranda outlines the salient reasons for\\nrecommending Foote, Cone & Belding\\'s \"Locker Room\"\\ncampaign for Kent family.\\n\\nThis is a unanimous decision among the Brand Group\\ninvolved and myself,\\n\\n \\n\\n/jf\\n\\nce: M. H. Burke\\nG. R. Telford\\nE. M. Kiernan\\n\\nS2z6LOTVO', 'oo Original Message-----\\n\\nFrom: Arce, Miguel\\n\\nSent: Monday, November 29, 1999 12:37 PM\\nTo: Bayliss, Elissa L.\\n\\nCe: Murillo, Joe; Garcia, Elisa\\n\\nSubject: MIAMI GRAND PRIX RACING SUBJECTS\\nImportance: High\\n\\nElissa:\\n\\nAs per your instructions of this morning attached please find samples of what was done with Indy transparencies\\nconcerning last years Miami Grand Prix promotion in Latin America. As described to you on my note of the 23rd, we would\\nlike to use new photos for posters, banners, counter displays and print (in Latin America only; Mexico, Central America,\\n\\nColombia, Ecuador and the Dominican Republic). Please reply as soon as possible as this is a time sensitive issue for us.\\nThank you for your help and support.\\n\\nBest regards,\\n\\nMiguel\\n\\nV0Z890L0202', 'Ww.\\n\\n3 T.\\n\\nWaggaman\\n\\nSmith\\n\\nMEMORANDUM\\nOctober 27,\\n\\n1992\\n\\nThis memo confirms the selection of Milwaukee, WI and\\nHartford, CT. as. True Micro-Lights test markets.\\n\\nThe Marketing, Sales, MPID and Media Departments have\\n\\napproved, these: markets. as suitable for True Micro-Lights\\n\\ntesting.\\n\\nMilwaukee will test the acetate filter product that. has\\n\\nalready been developed.\\nthe. Second Quarter,\\n\\nsg The \\xe2\\x80\\x94 the Hartford test gam on\\n\\nthe\\n\\nThe Milwaukee test. will begin in\\n1993. .\\n\\n \\n\\nAttached are results showing how Milwaukee and Hartford\\n\\nplace. on key measures affecting a. True Micro-Lights. test..\\n\\nIf you have any questions and/or comments please feel free\\n\\nto call.\\n\\n/kw\\n\\nAttachment\\n\\ncc:\\n\\ns.\\n\\nAugustyn\\nBenson\\nGiacoio\\nGoldstein\\nMcFadden\\nMcGlynn\\nMolloy\\nNicolaisen\\nPasheluk\\nSmith\\nTatulli\\nTelford\\n\\nAe\\n\\nSE6999z0\\n\\n \\n\\nog', \"-----Original Message----- _\\n\\nFrom: Bartlett, Dede\\n\\nSent: Tuesday, March 21, 2000 9:17 AM\\nTo: Logue, Mayada\\n\\nGc: Wallace, Karen\\n\\nSubject: RE: Women\\xe2\\x80\\x99s Sport's foundation\\n\\nyes...ifit's an early tunch...! have to leave for Concord, New Hampshire afterwards, Dede\\n\\nV96S8IEE8OT\", '-~---Original Message-----\\n\\nFrom: Suydam, Loraine\\n\\nSent: Thursday, January 18, 2001 4:05 PM\\nTo: Cohen, Denise A.: Kirschbaum, Marti N\\nSubject: Spectacular Stickers\\n\\nDeane Gross @ 3476 want to sheets of the spectacular stickers. It is for \"In the Loop\".\\n\\nLoraine Suydam\\n\\nYouth Smoking Prevention\\n(917)663-2252\\n(917)663-0449 Fax\\n\\n79\\n\\n96799862802', ':\\nAS 4\\n\\nKLevllard\\n\\n7 ee - MEMORANDUM\\nmo July 28, 1977\\n> Mr. R.-E. Smith\\nFROM: J. J. Giordano\\nRE: NEWPORT. BRAND -- Lights Expansion Market Plan\\n\\nAttached is a copy of subject plan for your information and\\ncomment. The overview includes all marketing aspects within the\\n\\n\\xe2\\x80\\x98currently established budget parameters. Should the brand re- -\\n\\nceive the previously requested additional dollar support, Lights\\nadvertising and promotion, as well as parent NEWPORT advertising\\nwould be increased. ,\\n\\nPlease advise should you have any questions or comments.\\n\\nJIG:hk - Attachment -\\n\\ncc: Messrs. Ave\\nBass - - - -\\nCunningham \\xe2\\x80\\x94 - -\\nFlinn\\nGiacoio\\n\\n- Gordon _\\nKeller\\nLawless\\nMastandrea\\nMueller \\xe2\\x80\\x94\\nToti\\n\\n  \\n \\n \\n \\n \\n \\n\\nRicci WGC (2)\\nHastings MCA', \"From:\\n\\nSubjects. .\\n\\nTotal\\nLBS.\\n\\n \\n\\n228,728\\n230,360)\\n221,625\\n209 ,845\\n205,646\\n217,063\\n84:,283\\n91,884\\n85.,900\\n86,412\\n72',722\\n84 ,224\\n\\n \\n\\n- Mr. W. G. Longest\\n\\nR. &. Grizzel\\n\\nChemical Composition of Stemmery, Samples, 19 aqCrop\\n~ BURLEY TOBACCO ( LIFT\\n\\nSample Identification\\n\\nDate\\n\\n2-19-81\\n2-19-81\\n2-20-81\\n2-20-81\\n2-23-81\\n2-23-81\\n2-19-81\\n2-19-81\\n2-20-81\\n2-20-81\\n2-23-81\\n\\n(2-23-81\\n\\nResults reported on 12% moisture basis.\\n\\nREG/bpp\\nDr. P. A. Eichorn\\n\\ncc:\\n\\nGrade\\n\\n \\n\\nB-3X\\nB-3X\\nB-3X\\nB-3X\\nB-3X\\nB-3X\\n\\nMKST Stem\\nMKST Stem\\nMKST Stem\\nMKST Stem\\nMKST Stem\\nMKST Stem\\n\\nMr. M. J. Buss\\n\\nMr.\\n\\nGellatly\\n\\nPHILIP MORRIS U. S. A.\\nINTER-OFFICE CORRESPONDENCE\\n\\nRICHMOND, VIRGINIA\\n\\n)\\n\\n% Total\\nAlkaloids. Nitrogen\\n\\n2.98)\\n\\n2.96\\n3.110\\n\\n3.01\\n\\n2.92\\n3.10\\n0.66\\n0.66\\n0.64\\n0.66\\n0.68\\n0.65\\n\\nAnalytical! Research File\\n\\n \\n\\nMarch 6, 1981\\n\\nDate:\\n\\n% Total\\nTVB\\n\\n. . .\\n. .\\n\\n+\\n\\noo 00 0 2&2\\n\\n7. 8 # #\\xc2\\xa2 8 - \\xc2\\xa9\\ne\\nvw ON ON NW ONS WS\\n\\nwow WwW wD WwW Ww WwW wD WwW WwW WwW\\n\\n    \\n  \\n\\n \\n\\n0.68\\n0.66\\n0.62\\n0.59\\n0.64\\n0.74\\n1.95\\n1.72\\n2.11\\n11.96\\n2.08 |\\n1.95 |\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWBsxyJWFRaw",
        "colab_type": "text"
      },
      "source": [
        "##1.5 Extracting keywords from a file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q-xhZXI0lIp",
        "colab_type": "code",
        "outputId": "45d46322-5b5d-4d15-9291-efa1ec2ee21e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "NERTestFilePath = '/content/extractedData/test/Email/2067227128.txt'\n",
        "with open(NERTestFilePath, 'r') as f:\n",
        "    sentence = f.read() \n",
        "\n",
        "print(sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Walk, Roger A.\n",
            "\n",
            "From: Walk, Roger A.\n",
            "\n",
            "Sent: Monday, July 22, 2002 12:27 PM\n",
            "To: Roethig, Hans; Koller, Debbie\n",
            "Subject: SCoR, clinical study 09/02\n",
            "\n",
            "Dear Hans and Debbie,\n",
            "\n",
            "In order to be prepared to any inquiries about this study to start in September with white packs of ‘U5’, Mike Pfeil suggests\n",
            "\n",
            "that a holding statement be prepared. Could you give me your facts input such as\n",
            "\n",
            "e Start and end of clinical trial\n",
            "\n",
            "Number and characteristics of subjects (e.g., gender, race, voluntary and reimbursed, pregnancy, age)\n",
            "Locations of trial\n",
            "\n",
            "CRO and principal investigator\n",
            "\n",
            "How is addressed that subjects are not exposed to increased risks\n",
            "\n",
            "How does the test smoking relate to the ‘normal’ smoking of the subjects?\n",
            "\n",
            "What is the purpose of the study?\n",
            "\n",
            "By the way: What arrangement is contracted with the GRO in case they obtain media inquiries?\n",
            "\n",
            "Regards,\n",
            "Roger\n",
            "\n",
            "8Z122zz90z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTfGc6B7Kl1T",
        "colab_type": "code",
        "outputId": "44ced698-f669-417f-d551-d5bbff37cb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl (29.8MB)\n",
            "\u001b[K     |████████████████████████████████| 29.8MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Collecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 46.6MB/s \n",
            "\u001b[?25hCollecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Collecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n",
            "Installing collected packages: srsly, wasabi, blis, thinc, spacy\n",
            "  Found existing installation: thinc 6.12.1\n",
            "    Uninstalling thinc-6.12.1:\n",
            "      Successfully uninstalled thinc-6.12.1\n",
            "  Found existing installation: spacy 2.0.18\n",
            "    Uninstalling spacy-2.0.18:\n",
            "      Successfully uninstalled spacy-2.0.18\n",
            "Successfully installed blis-0.2.4 spacy-2.1.4 srsly-0.0.5 thinc-7.0.4 wasabi-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cNRFsxYMl_0",
        "colab_type": "code",
        "outputId": "c2c0b804-fb8d-4fe1-fa31-25ddbf71e106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 608kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gqjck4np/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.0.0\n",
            "    Uninstalling en-core-web-sm-2.0.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.0.0\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnKxv78CLLNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from pprint import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2S0deAMMP26",
        "colab_type": "code",
        "outputId": "61351d63-a7d8-430f-ed99-16e696cd25ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "doc = nlp(sentence)\n",
        "##pprint([(X.text, X.label_) for X in doc.ents])\n",
        "pprint([(X.text) for X in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Roger A.\\n\\n',\n",
            " 'Walk',\n",
            " 'Roger A.\\n\\nSent',\n",
            " 'Monday, July 22, 2002',\n",
            " '12:27 PM',\n",
            " 'Hans',\n",
            " 'Koller',\n",
            " 'Debbie',\n",
            " 'SCoR',\n",
            " '09/02',\n",
            " 'Debbie',\n",
            " 'September',\n",
            " 'Mike Pfeil',\n",
            " 'GRO',\n",
            " 'Roger\\n\\n8Z122zz90z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etEDL91fPbLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}